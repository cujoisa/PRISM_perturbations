---
title: "Assess Classification Models"
author: "Matthew Berginski"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(here)
library(tidymodels)
library(ROCR)
library(patchwork)
library(tictoc)

knitr::opts_knit$set(root.dir = here())
```

# Convenience Functions

```{r}
process_full_roc_data <- function(all_results) {
	
	roc_vals = data.frame()
	for (model_id in unique(all_results$.config)) {
		these_results = all_results %>% filter(.config == model_id)
		
		pred <- prediction(these_results$.pred_TRUE,these_results$target_viability_split)
		perf_roc <- performance(pred, measure = "auc")
		prc_roc <- performance(pred, measure = "aucpr")
		
		roc_vals = bind_rows(
			roc_vals,
			data.frame(model_id = model_id,roc = perf_roc@y.values[[1]],prc = prc_roc@y.values[[1]] )
		)
	}
	
	roc_vals = roc_vals %>%
		mutate(roc_rank = dense_rank(desc(roc)),
					 prc_rank = dense_rank(desc(prc))) %>%
		arrange(roc_rank)
}

get_ROC_curve_values <- function(results) {
	pred <- prediction(results$.pred_TRUE,results$target_viability_split)
	perf <- performance(pred,measure = "tpr",x.measure = "fpr")
	return(data.frame(fpr = perf@x.values[[1]],
										tpr = perf@y.values[[1]]))
}

get_PRC_curve_values <- function(results) {
	pred <- prediction(results$.pred_TRUE,results$target_viability_split)
	perf <- performance(pred,measure = "prec",x.measure = "rec")
	return(data.frame(precision = perf@y.values[[1]],
										recall = perf@x.values[[1]]))
}
```

# Rand Forest Model Assessment

```{r}
tic()
if (file.exists(here('results/single_model/hyper_param_results.csv'))) {
	model_roc_prc = read_csv(here('results/single_model/hyper_param_results.csv'))
} else {
	model_roc_prc = data.frame()
	
	for (hyper in 1:100) {
		model_files = Sys.glob(here('results/single_model/rand_forest_param_scan/',sprintf('hyper%03d*.rds', hyper)))
		
		hyper_set_predictions = data.frame()
		for (this_file in model_files) {
			hyper_set_predictions = bind_rows(
				hyper_set_predictions,
				read_rds(this_file) %>% 
					collect_predictions() 
			)
		}
		
		model_roc_prc = bind_rows(
			model_roc_prc,
			process_full_roc_data(hyper_set_predictions) %>% mutate(hyper_set = hyper)
		)
		if (hyper %% 10 == 0) {
			print(sprintf('Done with %d\n',hyper))
		}
	}
	
	model_roc_prc = model_roc_prc %>% 
		mutate(prc_rank = min_rank(desc(prc)), 
					 roc_rank = min_rank(desc(roc)))
	
	write_csv(model_roc_prc,here('results/single_model/hyper_param_results.csv'))
}
toc()
```

```{r}
best_model_results = model_roc_prc %>% filter(roc_rank == 1)

model_files = Sys.glob(here('results/single_model/rand_forest_param_scan/',sprintf('hyper%03d*.rds', best_model_results$hyper_set)))

best_predictions = data.frame()
for (this_file in model_files) {
	best_predictions = bind_rows(
		best_predictions,
		read_rds(this_file) %>%
			collect_predictions()
	)
}

best_ROC = get_ROC_curve_values(best_predictions)
best_PRC = get_PRC_curve_values(best_predictions)
```

# Model Assessment/Visualization

```{r}
ROC_plots = ggplot(best_ROC, aes(x=fpr, y=tpr)) + 
	geom_abline(intercept = 0,slope = 1, alpha=0.5,linetype=2) +
	geom_line() + 
	# geom_text(mapping=aes(x=1,y=0,label=text),data=ROC_text,color='black',vjust="inward",hjust="inward",size=3.5) +
	# geom_segment(x=0,y=0,xend=1,yend=1) +
	xlim(c(0,1)) + ylim(c(0,1)) +
	labs(x="False Positive Rate",y="True Positive Rate", title = best_model_results$roc) +
	BerginskiRMisc::theme_berginski()

PRC_plots = ggplot(best_PRC, aes(x=recall, y=precision)) + 
	geom_abline(intercept = 0.5,slope = 0, alpha=0.5,linetype=2) +
	geom_line() + 
	labs(x="Recall",y="Precision", title = best_model_results$prc) +
	BerginskiRMisc::theme_berginski()

model_assess_plots = ROC_plots + PRC_plots
dir.create(here('figures/single_model/'), recursive = T, showWarnings = F)
ggsave(here('figures/single_model/roc_prc.png'), width=5,height=2.5)
BerginskiRMisc::trimImage(here('figures/single_model/roc_prc.png'))
```
